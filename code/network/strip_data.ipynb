{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "amended-illness",
   "metadata": {},
   "source": [
    "# Load and strip the original data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-junction",
   "metadata": {},
   "source": [
    "Original data was exported from the TU Graz online system by Susanne Voller between August and October 2021.  \n",
    "\n",
    "Data samples stored in the folder `data/original/data_samples` were supplied to the section \"Quality Management, Evaluation & Reporting\" of TU Graz, which is responsible for data protection issues. Based on these samples, the data was cleared for publication under the condition that student and lecturer identifiers (`student_id` and `lecturer_id`) are hashed.  \n",
    "\n",
    "In the following script, the raw files exported from the TU Graz online data base are read in and stripped of unneccessary columns. Student and lecturer identifiers are encoded with a salted hash function and original identifiers replaced by the hashed identifiers. The stripped ans hashed data is stored in the folder `data/raw` for further cleaning and imputation, which is performed in the script `clean_data.ipynb`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "great-there",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import join\n",
    "from bcrypt import gensalt, hashpw\n",
    "\n",
    "# parallelisation functionality\n",
    "from multiprocess import Pool\n",
    "import psutil\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "united-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../../data/original/data_exports\"\n",
    "dst = \"../../data/raw/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "brilliant-mandate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to hash the student and lecturer IDs given a randomly generated salt.\n",
    "def hash_id(ID):\n",
    "    hashed_id = hashpw(str(ID).encode('utf-8'), salt=salt)\n",
    "    return hashed_id[29:].decode()\n",
    "\n",
    "salt = gensalt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-surface",
   "metadata": {},
   "source": [
    "## Students"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-costs",
   "metadata": {},
   "source": [
    "Exported file from database: `Studiendaten.csv`.  \n",
    "Sample stored in `/origianl/data_samples/Studiendaten.csv`.\n",
    "\n",
    "**Original data fields & actions:**\n",
    "* `ST_PERSON_NR`: rename to `student_id` & hash\n",
    "* `STUDIENIDENTIFIKATOR`: rename to `study_id`\n",
    "* `STUDIENBEZEICHNUNG`: rename to `study_name`\n",
    "* `SEMESTERANZAHL`: rename to `term_number`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "protected-closer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24475/24475 [08:02<00:00, 50.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# List of studies for every student. A student can have more than one study,\n",
    "# which will show up as separate entries (row) for the same student_id. Each\n",
    "# study also has a term number, i.e. the number of semesters the student has\n",
    "# been enrolled in the given study.\n",
    "df = pd.read_csv(join(src, 'Studiendaten.csv'), encoding='latin_1')\n",
    "df = df.rename(columns={\n",
    "    'ST_PERSON_NR':'student_id', # unique student identifier\n",
    "    'STUDIENIDENTIFIKATOR':'study_id', # unique study identifier\n",
    "    'STUDIENBEZEICHNUNG':'study_name', # (german) name of the study\n",
    "    'SEMESTERANZAHL':'term_number' # number of terms a student has been enrolled\n",
    "})\n",
    "\n",
    "# hash student IDs with the given salt\n",
    "hashed_IDs = []\n",
    "pool = Pool(16)\n",
    "for hashed_ID in tqdm(\n",
    "        pool.imap_unordered(func=hash_id, iterable=df[\"student_id\"]),\n",
    "        total=len(df[\"student_id\"])\n",
    "    ):\n",
    "    hashed_IDs.append(hashed_ID)\n",
    "    \n",
    "df[\"student_id\"] = hashed_IDs   \n",
    "df.to_csv(join(dst, \"students.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-excerpt",
   "metadata": {},
   "source": [
    "## Lecturers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-argument",
   "metadata": {},
   "source": [
    "Exported file from database: `Bedienstete_mit_DV_an_Org.csv`  \n",
    "Sample stored in `/original/data_damples/Bedienstete_mit_DV_an_Org.csv`.  \n",
    "\n",
    "**Original data fields & actions:**\n",
    "* `PERSON_NR`: rename to `lecturer_id` & hash\n",
    "* `TUG_NEW.PUORG.GETNAME(A.ORG_NR)`: rename to `organisation_name`\n",
    "* `ORG_NR`: drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sharing-access",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5707/5707 [01:52<00:00, 50.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Mapping of lecturers to organisations (institute, faculty). A lecturer can\n",
    "# be associated with more than one organisation.\n",
    "df = pd.read_csv(join(src, 'Bedienstete_mit_DV_an_Org.csv'),\n",
    "                            encoding='latin_1')\n",
    "df = df.rename(columns={\n",
    "    'PERSON_NR':'lecturer_id', # unique lecturer id\n",
    "    'TUG_NEW.PUORG.GETNAME(A.ORG_NR)':'organisation_name' # German org name\n",
    "})\n",
    "\n",
    "df = df.drop(columns=[\n",
    "    \"ORG_NR\" # organisation ID, not needed\n",
    "])\n",
    "\n",
    "# hash lecturer IDs with the given salt\n",
    "hashed_IDs = []\n",
    "pool = Pool(16)\n",
    "for hashed_ID in tqdm(\n",
    "        pool.imap_unordered(func=hash_id, iterable=df[\"lecturer_id\"]),\n",
    "        total=len(df[\"lecturer_id\"])\n",
    "    ):\n",
    "    hashed_IDs.append(hashed_ID)\n",
    "    \n",
    "df[\"lecturer_id\"] = hashed_IDs\n",
    "df.to_csv(join(dst, \"lecturers.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-bearing",
   "metadata": {},
   "source": [
    "## Courses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diagnostic-laser",
   "metadata": {},
   "source": [
    "Exported file from database: `LV_cleaned.csv`.  \n",
    "Sample stored in `original/data_samples/LV_cleaned.csv`.   \n",
    "\n",
    "**Original data fields & actions:**\n",
    "* `STP_SP_NR`: rename to `course_id`\n",
    "* `STP_SP_TITEL_ENGL`: rename to `course_name`\n",
    "* `STP_LV_ART_KURZ`: rename to `course_type`, provide dictionary\n",
    "* `STP_SP_LVNR`: drop\n",
    "* `SJ_NAME`: drop\n",
    "* `SEMESTER_KB`: drop\n",
    "* `STP_SP_TITEL`: drop\n",
    "* `STP_SP_SST`: drop\n",
    "* `STP_LV_ART_NAME`: drop\n",
    "* `BETREUENDE_ORG_NR`: drop\n",
    "* `BETREUENDE_ORG_NAME`: drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bored-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of lectures with information about their type, their name, their module\n",
    "# (this is only relevant for how studies are composed at TU Graz) and the \n",
    "# organisational unit (institute, faculty) which is responsible for the lecture.\n",
    "\n",
    "# The list of lectures was manually cleaned, since for some rows, the entries \n",
    "# starting from the column STP_LV_ART_KURZ were shifted to the right by one \n",
    "# column. The originally exported file is \"/original/data_exports/LV.csv\".\n",
    "\n",
    "df = pd.read_csv(join(src, 'LV_cleaned.csv'), encoding=\"utf-8\")\n",
    "df = df.rename(columns={\n",
    "    'STP_SP_NR':'course_id', # unique course id\n",
    "    'STP_SP_TITEL_ENGL':'course_name', # english lecture name\n",
    "    'STP_LV_ART_KURZ':'course_type', # type of the lecture (tutorial, lab, ...)\n",
    "})\n",
    "df = df.drop(columns=[\n",
    "    \"SJ_NAME\",\n",
    "    \"SEMESTER_KB\",\n",
    "    \"STP_SP_SST\",\n",
    "    \"STP_LV_ART_NAME\",\n",
    "    \"STP_SP_TITEL\",\n",
    "    'STP_SP_LVNR',\n",
    "    'BETREUENDE_ORG_NR',\n",
    "    'BETREUENDE_ORG_NAME',\n",
    "    \"Unnamed: 11\"\n",
    "])\n",
    "df.to_csv(join(dst, \"courses.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-bridges",
   "metadata": {},
   "source": [
    "## Course enrollment by students"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-numbers",
   "metadata": {},
   "source": [
    "Exported file from database: `Studierende_pro_LV_mit_Idf.csv`.  \n",
    "Sample stored in `original/data_samples/Studierende_pro_LV_mit_Idf.csv`.\n",
    "\n",
    "**Original data fields & actions:**\n",
    "* `ST_PERSON_NR`: rename to `student_id` & hash\n",
    "* `STUDIENIDENTIFIKATOR`: rename to `study_id`\n",
    "* `STP_SP_NR`: rename to `course_id`\n",
    "* `LV_GRP_NR`: rename to `group_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sealed-midwest",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85505/85505 [28:50<00:00, 49.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# List of enrolled courses of the WiSe 2019/20 for every student. A course\n",
    "# can have several groups (for example for tutorial parts). The group\n",
    "# identifier is also listed for every student. It is not completely unique\n",
    "# as there are a number of overlapping groups (for example same time, \n",
    "# different rooms). These are disambiguated at a later point in the data\n",
    "# cleaning process.\n",
    "# The data also includes the identifier of the study through which the \n",
    "# student enrolled in a given lecture. \n",
    "df = pd.read_csv(join(src, 'Studierende_pro_LV_mit_Idf.csv'))\n",
    "df = df.rename(columns={\n",
    "    'ST_PERSON_NR':'student_id', # unique student identifier\n",
    "    'STUDIENIDENTIFIKATOR':'study_id', # unique study identifier\n",
    "    'STP_SP_NR':'course_id', # unique course identifier\n",
    "    'LV_GRP_NR':'group_id', # (almost) unique group identifier \n",
    "    }) \n",
    "\n",
    "# hash student IDs with the given salt\n",
    "hashed_IDs = []\n",
    "pool = Pool(16)\n",
    "for hashed_ID in tqdm(\n",
    "        pool.imap_unordered(func=hash_id, iterable=df[\"student_id\"]),\n",
    "        total=len(df[\"student_id\"])\n",
    "    ):\n",
    "    hashed_IDs.append(hashed_ID)\n",
    "    \n",
    "df[\"student_id\"] = hashed_IDs   \n",
    "df.to_csv(join(dst, \"course_enrollment.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-break",
   "metadata": {},
   "source": [
    "## Exam enrollment by students"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-athletics",
   "metadata": {},
   "source": [
    "Exported file from database: `Prüfungen-2.csv`.  \n",
    "Sample stored in `/original/data_exports/Prüfungen-2.csv`.\n",
    "\n",
    "**Original data fields & actions:**\n",
    "* `PV_TERM_NR`: rename to `exam_id`\n",
    "* `ST_PERSON_NR`: rename to `student_id` & hash\n",
    "* `STUDIENIDENTIFIKATOR`: rename to `study_id`\n",
    "* `STP_SP_NR`: rename to `course_id`\n",
    "* `PRUEFUNGSDATUM`: drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "backed-finnish",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57815/57815 [23:45<00:00, 40.57it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(join(src, 'Prüfungen-2.csv'), encoding='latin_1')\n",
    "df = df.rename(columns={\n",
    "    'PV_TERM_NR':'exam_id', # unique exam ID\n",
    "    'ST_PERSON_NR':'student_id', # unique student ID\n",
    "    'STUDIENIDENTIFIKATOR':'study_id', # unique study ID\n",
    "    'STP_SP_NR':'course_id'}) # unique course ID\n",
    "\n",
    "df = df.drop(columns=[\n",
    "    'PRUEFUNGSDATUM'\n",
    "])\n",
    "\n",
    "# hash student IDs with the given salt\n",
    "hashed_IDs = []\n",
    "pool = Pool(12)\n",
    "for hashed_ID in tqdm(\n",
    "        pool.imap_unordered(func=hash_id, iterable=df[\"student_id\"]),\n",
    "        total=len(df[\"student_id\"])\n",
    "    ):\n",
    "    hashed_IDs.append(hashed_ID)\n",
    "\n",
    "df[\"student_id\"] = hashed_IDs\n",
    "df.to_csv(join(dst, \"exam_enrollment.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-advocate",
   "metadata": {},
   "source": [
    "## Course supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-employer",
   "metadata": {},
   "source": [
    "Exported file from database `Lehrende.csv`.  \n",
    "Sample stored in `/original/data_samples/Lehrende.csv`.\n",
    "\n",
    "**Original data fields & actions:**\n",
    "* `PERSON_NR`: rename to `lecturer_id` & hash\n",
    "* `STP_SP_NR`: rename to `course_id`\n",
    "* `LV_GRP_NR`: rename to `group_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "brutal-vietnamese",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13212/13212 [05:21<00:00, 41.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# List of lecturers which are responsible for courses and groups within\n",
    "# courses. Similar to the list of students, the group_id is disambiguated\n",
    "# later in the data cleaning process.\n",
    "df = pd.read_csv(join(src, 'Lehrende.csv'))\n",
    "df = df.rename(columns={\n",
    "    'PERSON_NR':'lecturer_id', # unique lecturer id\n",
    "    'STP_SP_NR':'course_id', # unique lecture id\n",
    "    'LV_GRP_NR':'group_id'# (almost) unique group id\n",
    "})\n",
    "\n",
    "# hash lecturer IDs with the given salt\n",
    "hashed_IDs = []\n",
    "pool = Pool(12)\n",
    "for hashed_ID in tqdm(\n",
    "        pool.imap_unordered(func=hash_id, iterable=df[\"lecturer_id\"]),\n",
    "        total=len(df[\"lecturer_id\"])\n",
    "    ):\n",
    "    hashed_IDs.append(hashed_ID)\n",
    "    \n",
    "df[\"lecturer_id\"] = hashed_IDs\n",
    "df.to_csv(join(dst, \"course_supervision.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-farmer",
   "metadata": {},
   "source": [
    "## Exam supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-museum",
   "metadata": {},
   "source": [
    "Exported file from database `Prüfungstermine_mit_Räumen.csv`.  \n",
    "Sample stored in `/original/data_samples/Prüfungstermine_mit_Räumen.csv`.\n",
    "\n",
    "**Original data fields & actions:**\n",
    "* `PV_TERM_NR`: rename to `exam_id`\n",
    "* `PERSON_NR`: rename to `lecturer_id` & hash\n",
    "* `STP_SP_NR`: rename to `course_id`\n",
    "* `DATUM`: drop\n",
    "* `BEGINNZEIT`: drop\n",
    "* `ENDEZEIT`: drop\n",
    "* `RES_NR`: drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "restricted-conversation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5633/5633 [02:11<00:00, 42.77it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(join(src, 'Prüfungstermine_mit_Räumen.csv'), encoding='latin_1',\n",
    "                     parse_dates=['DATUM'], dayfirst=True)\n",
    "df = df.rename(columns={\n",
    "    'PV_TERM_NR':'exam_id', # unique exam ID\n",
    "    'PERSON_NR':'lecturer_id', # unique lecturer ID\n",
    "    'STP_SP_NR':'course_id'}) # unique course ID\n",
    "\n",
    "df = df.drop(columns=[\n",
    "    'DATUM',\n",
    "    'BEGINNZEIT',\n",
    "    'ENDEZEIT',\n",
    "    'RES_NR',\n",
    "])\n",
    "\n",
    "# hash lecturer IDs with the given salt\n",
    "hashed_IDs = []\n",
    "pool = Pool(12)\n",
    "for hashed_ID in tqdm(\n",
    "        pool.imap_unordered(func=hash_id, iterable=df[\"lecturer_id\"]),\n",
    "        total=len(df[\"lecturer_id\"])\n",
    "    ):\n",
    "    hashed_IDs.append(hashed_ID)\n",
    "    \n",
    "df[\"lecturer_id\"] = hashed_IDs\n",
    "df.to_csv(join(dst, \"exam_supervision.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-qatar",
   "metadata": {},
   "source": [
    "## Course dates and rooms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-stuart",
   "metadata": {},
   "source": [
    "Exported file from database: `Termine_mit_LV_Bezug.csv`.  \n",
    "Sample stored in `/original/data_samples/Termine_mit_LV_Bezug.csv`.\n",
    "\n",
    "**Original data fields & actions:**\n",
    "* `RES_NR`: rename to `room_id`\n",
    "* `DATUM_AM`: rename to `date`\n",
    "* `ZEIT_VON`: rename to `start_time`\n",
    "* `ZEIT_BIS`: rename to `end_time`\n",
    "* `STP_SP_NR`: rename to `course_id`\n",
    "* `LV_GRP_NR`: rename to `group_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "physical-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# events (start time, end time, room) for every course and group in WiSe 2019/20\n",
    "df = pd.read_csv(join(src, 'Termine_mit_LV_Bezug.csv'),\n",
    "                parse_dates=['DATUM_AM', 'ZEIT_VON', 'ZEIT_BIS'], dayfirst=True)\n",
    "df = df.rename(columns={\n",
    "    'RES_NR':'room_id', # unique room id\n",
    "    'DATUM_AM':'date', # date\n",
    "    'ZEIT_VON':'start_time', # start time\n",
    "    'ZEIT_BIS':'end_time', # end time\n",
    "    'STP_SP_NR':'course_id', # unique lecture id\n",
    "    'LV_GRP_NR':'group_id'# (almost) unique group id\n",
    "})\n",
    "df.to_csv(join(dst, \"course_dates.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-medicare",
   "metadata": {},
   "source": [
    "## Exam dates and rooms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-subscriber",
   "metadata": {},
   "source": [
    "Exported file from database `Prüfungstermine_mit_Räumen.csv`.  \n",
    "Sample stored in `Prüfungstermine_mit_Räumen.csv`.\n",
    "\n",
    "**Original data fields & actions:**\n",
    "* `PV_TERM_NR`: rename to `exam_id`\n",
    "* `DATUM`: rename to `date`\n",
    "* `BEGINNZEIT`: rename to `start_time`\n",
    "* `ENDEZEIT`: rename to `end_time`\n",
    "* `RES_NR`: rename to `room_id`\n",
    "* `STP_SP_NR`: rename to `course_id`\n",
    "* `PERSON_NR`: drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "disciplinary-seminar",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'lecturer_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/covid/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lecturer_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-cb44a4e3e5f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m for hashed_ID in tqdm(\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhash_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lecturer_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lecturer_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     ):\n",
      "\u001b[0;32m~/anaconda3/envs/covid/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/covid/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lecturer_id'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(join(src, 'Prüfungstermine_mit_Räumen.csv'), encoding='latin_1',\n",
    "                     parse_dates=['DATUM'], dayfirst=True)\n",
    "df = df.rename(columns={\n",
    "    'PV_TERM_NR':'exam_id', # unique exam ID\n",
    "    'DATUM':'date', # date of the exam\n",
    "    'BEGINNZEIT':'start_time', # start time of the exam\n",
    "    'ENDEZEIT':'end_time', # end time of the exam\n",
    "    'RES_NR':'room_id', # unique room ID\n",
    "    'STP_SP_NR':'course_id'}) # unique course ID\n",
    "\n",
    "df = df.drop(columns=[\n",
    "    'PERSON_NR',\n",
    "])\n",
    "\n",
    "# hash lecturer IDs with the given salt\n",
    "hashed_IDs = []\n",
    "pool = Pool(16)\n",
    "for hashed_ID in tqdm(\n",
    "        pool.imap_unordered(func=hash_id, iterable=df[\"lecturer_id\"]),\n",
    "        total=len(df[\"lecturer_id\"])\n",
    "    ):\n",
    "    hashed_IDs.append(hashed_ID)\n",
    "    \n",
    "df[\"lecturer_id\"] = hashed_IDs\n",
    "df.to_csv(join(dst, \"exam_dates.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-staff",
   "metadata": {},
   "source": [
    "## Rooms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-algeria",
   "metadata": {},
   "source": [
    "Exported file from database `Räume_cleaned.csv`.  \n",
    "Sample stored in `/original/data_samples/Räume_cleaned.csv`.\n",
    "\n",
    "**Original data fields & actions:**\n",
    "* `RES_NR`: rename to `room_id`\n",
    "* `RAUM_SITZPLAETZE`: rename to `seats`\n",
    "* `QUADRATMETER`: remame to `area`\n",
    "* `RAUM_GEBAEUDE_BEREICH_NAME`: rename to `campus`\n",
    "* `STRASSE`: rename to `address`\n",
    "* `PLZ`: rename to `postal_code`\n",
    "* `ORT`: rename to `city`\n",
    "* `RAUM_CODE`: drop\n",
    "* `RAUM_ZUSATZBEZEICHNUNG`: drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of rooms and information about them (number of seats, square meters).\n",
    "# TU Graz has three campuses: Alte Technik, Neue Technik and Inffeldgasse. The\n",
    "# mapping of every room to a campus is also stored.\n",
    "\n",
    "# Information for rooms outside TU Graz premises was missing. Jana Lasser \n",
    "# manually searched for and filled in room information for rooms at Uni Graz \n",
    "# and added the information to the file /data/raw/Räume.csv. The original file \n",
    "# is stored in /data/original/data_exports/Räume.csv. These rooms are excluded\n",
    "\n",
    "df = pd.read_csv(join(src, 'Räume_cleaned.csv'), \n",
    "                    encoding='latin_1')\n",
    "df = df.rename(columns={\n",
    "    'RES_NR':'room_id', # unique room id\n",
    "    'RAUM_SITZPLAETZE':'seats', # number of seats in the room\n",
    "    'QUADRATMETER':'area', # number of square meters in the room\n",
    "    'RAUM_GEBAEUDE_BEREICH_NAME':'campus', # campus where the room is located\n",
    "    'STRASSE':'address', # address (street & number)\n",
    "    'PLZ':'postal_code', # post code (always 8010)\n",
    "    'ORT':'city', # city (always Graz)\n",
    "})\n",
    "df = df.drop(columns=[\n",
    "    \"RAUM_CODE\",\n",
    "    \"RAUM_ZUSATZBEZEICHNUNG\"\n",
    "])\n",
    "df.to_csv(join(dst, \"rooms.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
